{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5291e15",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Estimate_fans_fast.py\n",
    "\n",
    "Reconstruct latent fan vote shares F_it for DWTS using:\n",
    "- softmax utility model\n",
    "- inequality-based likelihood from elimination events\n",
    "\n",
    "This is an optimized version that avoids pandas operations\n",
    "inside the likelihood, so SciPy optimization is much faster.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit  # sigmoid\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load DWTS data\n",
    "# ============================================================\n",
    "\n",
    "DATA_PATH = \"2026_MCM_Problem_C_Data.csv\"  # change if needed\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Number of rows (contestant-season):\", len(df_raw))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Helpers: parse result strings, find week columns\n",
    "# ============================================================\n",
    "\n",
    "def parse_elim_week(result_str: str):\n",
    "    \"\"\"\n",
    "    Parse 'results' string to get elimination week.\n",
    "    Examples:\n",
    "        'Eliminated Week 3' -> 3\n",
    "        '1st Place'         -> None\n",
    "        '2nd Place'         -> None\n",
    "    \"\"\"\n",
    "    if not isinstance(result_str, str):\n",
    "        return None\n",
    "    m = re.search(r\"Eliminated Week\\s+(\\d+)\", result_str)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_week_judge_cols(columns):\n",
    "    \"\"\"\n",
    "    From all column names, find 'weekX_judgeY_score' columns.\n",
    "    Return: dict week -> [list of columns for that week]\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"week(\\d+)_judge(\\d+)_score\")\n",
    "    week_to_cols = {}\n",
    "    for col in columns:\n",
    "        m = pattern.match(col)\n",
    "        if m:\n",
    "            w = int(m.group(1))\n",
    "            week_to_cols.setdefault(w, []).append(col)\n",
    "    # sort judge cols per week for consistency\n",
    "    for w in week_to_cols:\n",
    "        week_to_cols[w] = sorted(week_to_cols[w])\n",
    "    return week_to_cols\n",
    "\n",
    "\n",
    "week_to_cols = collect_week_judge_cols(df_raw.columns)\n",
    "print(\"Detected weeks:\", sorted(week_to_cols.keys()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Build long panel: one row per (season, week, contestant)\n",
    "# ============================================================\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, row in df_raw.iterrows():\n",
    "    season = int(row[\"season\"])\n",
    "    celeb = row[\"celebrity_name\"]\n",
    "    elim_week = parse_elim_week(row[\"results\"])\n",
    "\n",
    "    for week, cols in week_to_cols.items():\n",
    "        scores = row[cols].values.astype(float)\n",
    "\n",
    "        # if all NaN → contestant not active / show not started\n",
    "        if np.all(np.isnan(scores)):\n",
    "            continue\n",
    "\n",
    "        total_judge = np.nansum(scores)\n",
    "\n",
    "        if elim_week is None:\n",
    "            alive = True\n",
    "            eliminated = False\n",
    "        else:\n",
    "            alive = week <= elim_week\n",
    "            eliminated = (week == elim_week)\n",
    "\n",
    "        if not alive:\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"season\": season,\n",
    "                \"celebrity_name\": celeb,\n",
    "                \"week\": week,\n",
    "                \"judge_total\": total_judge,\n",
    "                \"elim_week\": elim_week,\n",
    "                \"eliminated\": int(eliminated),\n",
    "            }\n",
    "        )\n",
    "\n",
    "panel = pd.DataFrame(rows)\n",
    "print(\"Panel shape:\", panel.shape)\n",
    "print(panel.head())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Standardize judge scores within (season, week)\n",
    "# ============================================================\n",
    "\n",
    "panel[\"judge_std\"] = (\n",
    "    panel.groupby([\"season\", \"week\"])[\"judge_total\"]\n",
    "    .transform(lambda s: (s - s.mean()) / s.std(ddof=0))\n",
    ")\n",
    "panel[\"judge_std\"] = panel[\"judge_std\"].fillna(0.0)  # all-equal scores\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Encode contestant IDs and weeks, precompute arrays\n",
    "# ============================================================\n",
    "\n",
    "# contestant_id: unique per (season, celebrity_name)\n",
    "panel[\"contestant_id\"] = panel.groupby([\"season\", \"celebrity_name\"]).ngroup()\n",
    "panel[\"week_float\"] = panel[\"week\"].astype(float)\n",
    "\n",
    "n_contestants = panel[\"contestant_id\"].max() + 1\n",
    "print(\"Number of contestant-season units:\", n_contestants)\n",
    "\n",
    "# Numpy views aligned with panel.index\n",
    "cid_arr = panel[\"contestant_id\"].to_numpy()\n",
    "week_arr = panel[\"week_float\"].to_numpy()\n",
    "judge_total_arr = panel[\"judge_total\"].to_numpy()\n",
    "judge_std_arr = panel[\"judge_std\"].to_numpy()\n",
    "\n",
    "season_arr = panel[\"season\"].to_numpy()\n",
    "elim_arr = panel[\"eliminated\"].to_numpy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Build elimination events with row indices (no pandas in likelihood)\n",
    "# ============================================================\n",
    "\n",
    "events = []\n",
    "\n",
    "for (season, week), sub in panel.groupby([\"season\", \"week\"]):\n",
    "    elim_rows = sub[sub[\"eliminated\"] == 1]\n",
    "    if len(elim_rows) != 1:\n",
    "        continue  # skip weeks with 0 or >1 eliminations\n",
    "    elim_row_index = elim_rows.index[0]\n",
    "    row_idx = sub.index.to_numpy()  # all alive contestants this week\n",
    "\n",
    "    # position of eliminated contestant within row_idx\n",
    "    elim_pos = int(np.where(row_idx == elim_row_index)[0][0])\n",
    "\n",
    "    events.append(\n",
    "        {\n",
    "            \"row_idx\": row_idx,  # indices into panel\n",
    "            \"elim_pos\": elim_pos,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Number of usable elimination events:\", len(events))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Model config and parameter unpacking\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    alpha_soft: float = 10.0  # steepness of sigmoid constraint\n",
    "    use_week_effect: bool = True\n",
    "\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# θ = [a_0,...,a_{n-1}, β_T, β_week(optional)]\n",
    "n_params = n_contestants + (2 if config.use_week_effect else 1)\n",
    "\n",
    "\n",
    "def unpack_theta(theta):\n",
    "    \"\"\"Split flat parameter vector into (a, beta_T, beta_week).\"\"\"\n",
    "    a = theta[:n_contestants]\n",
    "    beta_T = theta[n_contestants]\n",
    "    if config.use_week_effect:\n",
    "        beta_week = theta[n_contestants + 1]\n",
    "    else:\n",
    "        beta_week = 0.0\n",
    "    return a, beta_T, beta_week\n",
    "\n",
    "\n",
    "def compute_eta(theta):\n",
    "    \"\"\"\n",
    "    Compute η_{it} = a_i + β_T * judge_std + β_week * week\n",
    "    for every row in panel.\n",
    "    Returns numpy array of shape (len(panel),).\n",
    "    \"\"\"\n",
    "    a, beta_T, beta_week = unpack_theta(theta)\n",
    "    eta = a[cid_arr] + beta_T * judge_std_arr\n",
    "    if config.use_week_effect:\n",
    "        eta = eta + beta_week * week_arr\n",
    "    return eta\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Standard softmax, numerically stable.\"\"\"\n",
    "    z = x - np.max(x)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / exp_z.sum()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Negative log-likelihood using inequality constraints\n",
    "# ============================================================\n",
    "\n",
    "def neg_log_likelihood(theta):\n",
    "    \"\"\"\n",
    "    For each elimination event:\n",
    "        - compute fan shares via softmax(η)\n",
    "        - form total score S_i = J_pct_i + F_i (percentage scheme)\n",
    "        - penalize if eliminated contestant does NOT have the lowest S_i\n",
    "    The penalty is implemented as sum of log(sigmoid(alpha * (S_j - S_elim)))\n",
    "    for all survivors j.\n",
    "    \"\"\"\n",
    "    eta_all = compute_eta(theta)\n",
    "    loglike = 0.0\n",
    "    eps = 1e-12\n",
    "\n",
    "    for ev in events:\n",
    "        idx = ev[\"row_idx\"]          # indices of alive contestants this week\n",
    "        elim_pos = ev[\"elim_pos\"]    # which position is eliminated\n",
    "\n",
    "        eta_week = eta_all[idx]      # η for these contestants\n",
    "        F_week = softmax(eta_week)   # fan shares among alive contestants\n",
    "\n",
    "        J_week = judge_total_arr[idx]\n",
    "        J_pct = J_week / J_week.sum()\n",
    "\n",
    "        S = J_pct + F_week\n",
    "        S_elim = S[elim_pos]\n",
    "\n",
    "        # survivors indices (within this week)\n",
    "        mask_survivor = np.ones_like(S, dtype=bool)\n",
    "        mask_survivor[elim_pos] = False\n",
    "        S_surv = S[mask_survivor]\n",
    "\n",
    "        diff = S_surv - S_elim  # we want these to be >= 0\n",
    "        p_ok = expit(config.alpha_soft * diff)\n",
    "        loglike += np.sum(np.log(p_ok + eps))\n",
    "\n",
    "    return -loglike  # minimizer wants negative log-likelihood\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Fit the model with SciPy\n",
    "# ============================================================\n",
    "\n",
    "# Initial guess: a_i = 0, β_T = 1, β_week = 0\n",
    "theta0 = np.zeros(n_params)\n",
    "theta0[n_contestants] = 1.0  # β_T\n",
    "\n",
    "result = minimize(\n",
    "    neg_log_likelihood,\n",
    "    theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\"maxiter\": 80},  # you can increase if you want\n",
    ")\n",
    "\n",
    "print(\"Optimization success:\", result.success)\n",
    "print(\"Message:\", result.message)\n",
    "print(\"Final negative log-likelihood:\", result.fun)\n",
    "\n",
    "theta_hat = result.x\n",
    "a_hat, beta_T_hat, beta_week_hat = unpack_theta(theta_hat)\n",
    "print(\"beta_T (effect of judge_std):\", beta_T_hat)\n",
    "print(\"beta_week:\", beta_week_hat)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. Reconstruct fan shares F_hat for each (season, week, contestant)\n",
    "# ============================================================\n",
    "\n",
    "eta_hat = compute_eta(theta_hat)\n",
    "\n",
    "fan_rows = []\n",
    "for (season, week), sub in panel.groupby([\"season\", \"week\"]):\n",
    "    idx = sub.index.to_numpy()\n",
    "    eta_week = eta_hat[idx]\n",
    "    F_week = softmax(eta_week)\n",
    "    for row_idx, F_i in zip(idx, F_week):\n",
    "        fan_rows.append(\n",
    "            {\n",
    "                \"row_index\": row_idx,\n",
    "                \"fan_share_hat\": F_i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "F_df = pd.DataFrame(fan_rows).set_index(\"row_index\")\n",
    "panel[\"fan_share_hat\"] = F_df[\"fan_share_hat\"]\n",
    "\n",
    "print(panel.head())\n",
    "\n",
    "# Optionally save the reconstructed panel for later questions\n",
    "panel.to_csv(\"fan_shares_estimated.csv\", index=False)\n",
    "print(\"Saved fan_shares_estimated.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimate_fans_fast.py\n",
    "\n",
    "Reconstruct latent fan vote shares F_it for DWTS using:\n",
    "- softmax utility model\n",
    "- inequality-based likelihood from elimination events\n",
    "\n",
    "This is an optimized version that avoids pandas operations\n",
    "inside the likelihood, so SciPy optimization is much faster.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit  # sigmoid\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load DWTS data\n",
    "# ============================================================\n",
    "\n",
    "DATA_PATH = \"2026_MCM_Problem_C_Data.csv\"  # change if needed\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Number of rows (contestant-season):\", len(df_raw))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Helpers: parse result strings, find week columns\n",
    "# ============================================================\n",
    "\n",
    "def parse_elim_week(result_str: str):\n",
    "    \"\"\"\n",
    "    Parse 'results' string to get elimination week.\n",
    "    Examples:\n",
    "        'Eliminated Week 3' -> 3\n",
    "        '1st Place'         -> None\n",
    "        '2nd Place'         -> None\n",
    "    \"\"\"\n",
    "    if not isinstance(result_str, str):\n",
    "        return None\n",
    "    m = re.search(r\"Eliminated Week\\s+(\\d+)\", result_str)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_week_judge_cols(columns):\n",
    "    \"\"\"\n",
    "    From all column names, find 'weekX_judgeY_score' columns.\n",
    "    Return: dict week -> [list of columns for that week]\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"week(\\d+)_judge(\\d+)_score\")\n",
    "    week_to_cols = {}\n",
    "    for col in columns:\n",
    "        m = pattern.match(col)\n",
    "        if m:\n",
    "            w = int(m.group(1))\n",
    "            week_to_cols.setdefault(w, []).append(col)\n",
    "    # sort judge cols per week for consistency\n",
    "    for w in week_to_cols:\n",
    "        week_to_cols[w] = sorted(week_to_cols[w])\n",
    "    return week_to_cols\n",
    "\n",
    "\n",
    "week_to_cols = collect_week_judge_cols(df_raw.columns)\n",
    "print(\"Detected weeks:\", sorted(week_to_cols.keys()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Build long panel: one row per (season, week, contestant)\n",
    "# ============================================================\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, row in df_raw.iterrows():\n",
    "    season = int(row[\"season\"])\n",
    "    celeb = row[\"celebrity_name\"]\n",
    "    elim_week = parse_elim_week(row[\"results\"])\n",
    "\n",
    "    for week, cols in week_to_cols.items():\n",
    "        scores = row[cols].values.astype(float)\n",
    "\n",
    "        # if all NaN → contestant not active / show not started\n",
    "        if np.all(np.isnan(scores)):\n",
    "            continue\n",
    "\n",
    "        total_judge = np.nansum(scores)\n",
    "\n",
    "        if elim_week is None:\n",
    "            alive = True\n",
    "            eliminated = False\n",
    "        else:\n",
    "            alive = week <= elim_week\n",
    "            eliminated = (week == elim_week)\n",
    "\n",
    "        if not alive:\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"season\": season,\n",
    "                \"celebrity_name\": celeb,\n",
    "                \"week\": week,\n",
    "                \"judge_total\": total_judge,\n",
    "                \"elim_week\": elim_week,\n",
    "                \"eliminated\": int(eliminated),\n",
    "            }\n",
    "        )\n",
    "\n",
    "panel = pd.DataFrame(rows)\n",
    "print(\"Panel shape:\", panel.shape)\n",
    "print(panel.head())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Standardize judge scores within (season, week)\n",
    "# ============================================================\n",
    "\n",
    "panel[\"judge_std\"] = (\n",
    "    panel.groupby([\"season\", \"week\"])[\"judge_total\"]\n",
    "    .transform(lambda s: (s - s.mean()) / s.std(ddof=0))\n",
    ")\n",
    "panel[\"judge_std\"] = panel[\"judge_std\"].fillna(0.0)  # all-equal scores\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Encode contestant IDs and weeks, precompute arrays\n",
    "# ============================================================\n",
    "\n",
    "# contestant_id: unique per (season, celebrity_name)\n",
    "panel[\"contestant_id\"] = panel.groupby([\"season\", \"celebrity_name\"]).ngroup()\n",
    "panel[\"week_float\"] = panel[\"week\"].astype(float)\n",
    "\n",
    "n_contestants = panel[\"contestant_id\"].max() + 1\n",
    "print(\"Number of contestant-season units:\", n_contestants)\n",
    "\n",
    "# Numpy views aligned with panel.index\n",
    "cid_arr = panel[\"contestant_id\"].to_numpy()\n",
    "week_arr = panel[\"week_float\"].to_numpy()\n",
    "judge_total_arr = panel[\"judge_total\"].to_numpy()\n",
    "judge_std_arr = panel[\"judge_std\"].to_numpy()\n",
    "\n",
    "season_arr = panel[\"season\"].to_numpy()\n",
    "elim_arr = panel[\"eliminated\"].to_numpy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Build elimination events with row indices (no pandas in likelihood)\n",
    "# ============================================================\n",
    "\n",
    "events = []\n",
    "\n",
    "for (season, week), sub in panel.groupby([\"season\", \"week\"]):\n",
    "    elim_rows = sub[sub[\"eliminated\"] == 1]\n",
    "    if len(elim_rows) != 1:\n",
    "        continue  # skip weeks with 0 or >1 eliminations\n",
    "    elim_row_index = elim_rows.index[0]\n",
    "    row_idx = sub.index.to_numpy()  # all alive contestants this week\n",
    "\n",
    "    # position of eliminated contestant within row_idx\n",
    "    elim_pos = int(np.where(row_idx == elim_row_index)[0][0])\n",
    "\n",
    "    events.append(\n",
    "        {\n",
    "            \"row_idx\": row_idx,  # indices into panel\n",
    "            \"elim_pos\": elim_pos,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Number of usable elimination events:\", len(events))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Model config and parameter unpacking\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    alpha_soft: float = 10.0  # steepness of sigmoid constraint\n",
    "    use_week_effect: bool = True\n",
    "\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# θ = [a_0,...,a_{n-1}, β_T, β_week(optional)]\n",
    "n_params = n_contestants + (2 if config.use_week_effect else 1)\n",
    "\n",
    "\n",
    "def unpack_theta(theta):\n",
    "    \"\"\"Split flat parameter vector into (a, beta_T, beta_week).\"\"\"\n",
    "    a = theta[:n_contestants]\n",
    "    beta_T = theta[n_contestants]\n",
    "    if config.use_week_effect:\n",
    "        beta_week = theta[n_contestants + 1]\n",
    "    else:\n",
    "        beta_week = 0.0\n",
    "    return a, beta_T, beta_week\n",
    "\n",
    "\n",
    "def compute_eta(theta):\n",
    "    \"\"\"\n",
    "    Compute η_{it} = a_i + β_T * judge_std + β_week * week\n",
    "    for every row in panel.\n",
    "    Returns numpy array of shape (len(panel),).\n",
    "    \"\"\"\n",
    "    a, beta_T, beta_week = unpack_theta(theta)\n",
    "    eta = a[cid_arr] + beta_T * judge_std_arr\n",
    "    if config.use_week_effect:\n",
    "        eta = eta + beta_week * week_arr\n",
    "    return eta\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Standard softmax, numerically stable.\"\"\"\n",
    "    z = x - np.max(x)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / exp_z.sum()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Negative log-likelihood using inequality constraints\n",
    "# ============================================================\n",
    "\n",
    "def neg_log_likelihood(theta):\n",
    "    \"\"\"\n",
    "    For each elimination event:\n",
    "        - compute fan shares via softmax(η)\n",
    "        - form total score S_i = J_pct_i + F_i (percentage scheme)\n",
    "        - penalize if eliminated contestant does NOT have the lowest S_i\n",
    "    The penalty is implemented as sum of log(sigmoid(alpha * (S_j - S_elim)))\n",
    "    for all survivors j.\n",
    "    \"\"\"\n",
    "    eta_all = compute_eta(theta)\n",
    "    loglike = 0.0\n",
    "    eps = 1e-12\n",
    "\n",
    "    for ev in events:\n",
    "        idx = ev[\"row_idx\"]          # indices of alive contestants this week\n",
    "        elim_pos = ev[\"elim_pos\"]    # which position is eliminated\n",
    "\n",
    "        eta_week = eta_all[idx]      # η for these contestants\n",
    "        F_week = softmax(eta_week)   # fan shares among alive contestants\n",
    "\n",
    "        J_week = judge_total_arr[idx]\n",
    "        J_pct = J_week / J_week.sum()\n",
    "\n",
    "        S = J_pct + F_week\n",
    "        S_elim = S[elim_pos]\n",
    "\n",
    "        # survivors indices (within this week)\n",
    "        mask_survivor = np.ones_like(S, dtype=bool)\n",
    "        mask_survivor[elim_pos] = False\n",
    "        S_surv = S[mask_survivor]\n",
    "\n",
    "        diff = S_surv - S_elim  # we want these to be >= 0\n",
    "        p_ok = expit(config.alpha_soft * diff)\n",
    "        loglike += np.sum(np.log(p_ok + eps))\n",
    "\n",
    "    return -loglike  # minimizer wants negative log-likelihood\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Fit the model with SciPy\n",
    "# ============================================================\n",
    "\n",
    "# Initial guess: a_i = 0, β_T = 1, β_week = 0\n",
    "theta0 = np.zeros(n_params)\n",
    "theta0[n_contestants] = 1.0  # β_T\n",
    "\n",
    "result = minimize(\n",
    "    neg_log_likelihood,\n",
    "    theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\"maxiter\": 80},  # you can increase if you want\n",
    ")\n",
    "\n",
    "print(\"Optimization success:\", result.success)\n",
    "print(\"Message:\", result.message)\n",
    "print(\"Final negative log-likelihood:\", result.fun)\n",
    "\n",
    "theta_hat = result.x\n",
    "a_hat, beta_T_hat, beta_week_hat = unpack_theta(theta_hat)\n",
    "print(\"beta_T (effect of judge_std):\", beta_T_hat)\n",
    "print(\"beta_week:\", beta_week_hat)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. Reconstruct fan shares F_hat for each (season, week, contestant)\n",
    "# ============================================================\n",
    "\n",
    "eta_hat = compute_eta(theta_hat)\n",
    "\n",
    "fan_rows = []\n",
    "for (season, week), sub in panel.groupby([\"season\", \"week\"]):\n",
    "    idx = sub.index.to_numpy()\n",
    "    eta_week = eta_hat[idx]\n",
    "    F_week = softmax(eta_week)\n",
    "    for row_idx, F_i in zip(idx, F_week):\n",
    "        fan_rows.append(\n",
    "            {\n",
    "                \"row_index\": row_idx,\n",
    "                \"fan_share_hat\": F_i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "F_df = pd.DataFrame(fan_rows).set_index(\"row_index\")\n",
    "panel[\"fan_share_hat\"] = F_df[\"fan_share_hat\"]\n",
    "\n",
    "print(panel.head())\n",
    "\n",
    "# Optionally save the reconstructed panel for later questions\n",
    "panel.to_csv(\"fan_shares_estimated.csv\", index=False)\n",
    "print(\"Saved fan_shares_estimated.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00974c",
   "metadata": {},
   "source": [
    "## Consistency of Reconstructed Fan Votes with Historical Eliminations\n",
    "\n",
    "This script evaluates how well the reconstructed fan vote shares reproduce the actual elimination outcomes across all seasons.\n",
    "\n",
    "Using the file `fan_shares_estimated.csv` (which contains judges’ totals, reconstructed fan shares, and elimination indicators), we:\n",
    "\n",
    "- Restrict to weeks with **exactly one eliminated contestant** and at least two active contestants.\n",
    "- Simulate eliminations under two combination rules:\n",
    "  - **Rank-Sum** (used in seasons 1–2 and 28–34):  \n",
    "    Rank contestants separately by judges’ total and fan share (1 = best), sum the ranks, and eliminate the contestant with the **largest** combined rank.\n",
    "  - **Percent-Sum** (used in seasons 3–27):  \n",
    "    Convert judges’ totals to weekly percentages, add reconstructed fan share percentages, and eliminate the contestant with the **lowest** combined percentage.\n",
    "- For each elimination event, compute:\n",
    "  - **Hit rate**: whether the predicted eliminated contestant matches the actual one.\n",
    "  - **Satisfaction rate**: what fraction of competitors are “more deserving to stay” than the eliminated contestant under the chosen rule.\n",
    "  - **Worst rank / percentile**: how poorly the eliminated contestant ranks in that week under the rule.\n",
    "\n",
    "The script outputs:\n",
    "- `verification_event_level.csv`: event-level diagnostics per (season, week).\n",
    "- `verification_season_level.csv`: season-level summaries of accuracy and consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e14360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "consistency_evaluation.py\n",
    "\n",
    "Evaluate how consistent the reconstructed fan shares are with the\n",
    "actual DWTS elimination outcomes under different scoring schemes.\n",
    "\n",
    "Input:\n",
    "    EST_PATH (CSV): panel with reconstructed fan shares, expected columns:\n",
    "        - season\n",
    "        - week\n",
    "        - celebrity_name\n",
    "        - judge_total\n",
    "        - eliminated  (1 if eliminated in that week, else 0)\n",
    "        - fan_share_hat\n",
    "\n",
    "Output:\n",
    "    verification_event_level.csv   : event-level metrics for each (season, week)\n",
    "    verification_season_level.csv  : season-level summary metrics\n",
    "\n",
    "Scoring schemes:\n",
    "    - Rank-Sum (seasons 1, 2, 28–34)\n",
    "    - Percent-Sum (seasons 3–27)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ======================================================================\n",
    "# Paths (change EST_PATH if needed)\n",
    "# ======================================================================\n",
    "EST_PATH = \"/content/fan_shares_estimated.csv\"\n",
    "OUT_EVENT_PATH = \"verification_event_level.csv\"\n",
    "OUT_SEASON_PATH = \"verification_season_level.csv\"\n",
    "\n",
    "# Seasons using Rank-Sum combination in the actual show\n",
    "RANK_SUM_SEASONS = set([1, 2] + list(range(28, 35)))\n",
    "\n",
    "\n",
    "def load_panel(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the reconstructed panel and validate required columns.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    required_cols = {\n",
    "        \"season\",\n",
    "        \"week\",\n",
    "        \"celebrity_name\",\n",
    "        \"judge_total\",\n",
    "        \"eliminated\",\n",
    "        \"fan_share_hat\",\n",
    "    }\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in estimated panel: {missing}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_usable_events(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Keep only (season, week) pairs with exactly one eliminated contestant\n",
    "    and at least two active contestants. This mirrors the modeling\n",
    "    assumptions used when building the elimination events.\n",
    "    \"\"\"\n",
    "    g = df.groupby([\"season\", \"week\"], sort=True)\n",
    "    usable_keys = []\n",
    "    for (s, w), sub in g:\n",
    "        if sub[\"eliminated\"].sum() == 1 and len(sub) >= 2:\n",
    "            usable_keys.append((s, w))\n",
    "\n",
    "    print(\"Usable elimination events:\", len(usable_keys))\n",
    "    return g, usable_keys\n",
    "\n",
    "\n",
    "def rank_sum_elim(sub: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Rank-Sum rule:\n",
    "\n",
    "    - Rank contestants by judge_total (descending) and by fan_share_hat (descending),\n",
    "      with rank 1 = best.\n",
    "    - Combined rank = judge_rank + fan_rank (smaller = better).\n",
    "    - Predicted eliminated contestant = one with the largest combined rank (worst).\n",
    "\n",
    "    Returns:\n",
    "        pred_name      : predicted eliminated contestant name\n",
    "        act_name       : actual eliminated contestant name\n",
    "        sat_rate       : fraction of other contestants whose combined rank\n",
    "                         is <= the eliminated contestant's combined rank\n",
    "                         (should be close to 1 if the eliminated contestant\n",
    "                         is truly the worst).\n",
    "        margin         : comb_elim - max_other_comb (>= 0 if eliminated is worst)\n",
    "        worst_rank     : \"worst-first\" rank of the actual eliminated contestant\n",
    "                         (1 = worst)\n",
    "        n_alive        : number of active contestants in this event\n",
    "    \"\"\"\n",
    "    sub = sub.reset_index(drop=True).copy()\n",
    "\n",
    "    j = sub[\"judge_total\"].to_numpy(dtype=float)\n",
    "    f = sub[\"fan_share_hat\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Normalize fan shares within this week for safety\n",
    "    f = f / f.sum() if f.sum() > 0 else np.ones_like(f) / len(f)\n",
    "\n",
    "    # Rank: 1 = best (larger value = better)\n",
    "    judge_rank = pd.Series(-j).rank(method=\"min\").to_numpy()\n",
    "    fan_rank = pd.Series(-f).rank(method=\"min\").to_numpy()\n",
    "\n",
    "    comb = judge_rank + fan_rank  # smaller = better\n",
    "    pred_idx = int(np.argmax(comb))  # largest combined rank -> worst\n",
    "    act_idx = int(np.where(sub[\"eliminated\"].to_numpy() == 1)[0][0])\n",
    "\n",
    "    pred_name = sub.loc[pred_idx, \"celebrity_name\"]\n",
    "    act_name = sub.loc[act_idx, \"celebrity_name\"]\n",
    "\n",
    "    # Consistency diagnostics\n",
    "    # sat_rate: fraction of others with comb <= comb_elim\n",
    "    sat_rate = float(np.mean(np.delete(comb, act_idx) <= comb[act_idx]))\n",
    "\n",
    "    # margin: comb_elim - max_other_comb (>= 0 if eliminated is truly worst)\n",
    "    margin = float(comb[act_idx] - np.max(np.delete(comb, act_idx)))\n",
    "\n",
    "    # worst_rank: rank of actual eliminated in \"worst-first\" order (1 = worst)\n",
    "    worst_rank = float(\n",
    "        pd.Series(comb).rank(method=\"min\", ascending=False).to_numpy()[act_idx]\n",
    "    )\n",
    "\n",
    "    return pred_name, act_name, sat_rate, margin, worst_rank, len(sub)\n",
    "\n",
    "\n",
    "def pct_sum_elim(sub: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Percent-Sum rule:\n",
    "\n",
    "    - J_pct = judge_total / sum(judge_total) (judge percentage within the week).\n",
    "    - F_pct = normalized fan_share_hat within the same week.\n",
    "    - Combined score S = J_pct + F_pct.\n",
    "    - Predicted eliminated contestant = one with smallest S (worst total).\n",
    "\n",
    "    Returns:\n",
    "        pred_name, act_name, sat_rate, margin, worst_rank, n_alive\n",
    "        with analogous definitions to rank_sum_elim.\n",
    "    \"\"\"\n",
    "    sub = sub.reset_index(drop=True).copy()\n",
    "\n",
    "    j = sub[\"judge_total\"].to_numpy(dtype=float)\n",
    "    f = sub[\"fan_share_hat\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Normalize fan shares within this week for safety\n",
    "    f = f / f.sum() if f.sum() > 0 else np.ones_like(f) / len(f)\n",
    "\n",
    "    j_sum = j.sum()\n",
    "    j_pct = (j / j_sum) if j_sum > 0 else (np.ones_like(j) / len(j))\n",
    "\n",
    "    S = j_pct + f  # larger = better\n",
    "    pred_idx = int(np.argmin(S))  # smallest S -> worst\n",
    "    act_idx = int(np.where(sub[\"eliminated\"].to_numpy() == 1)[0][0])\n",
    "\n",
    "    pred_name = sub.loc[pred_idx, \"celebrity_name\"]\n",
    "    act_name = sub.loc[act_idx, \"celebrity_name\"]\n",
    "\n",
    "    # sat_rate: fraction of others with S >= S_elim (should be close to 1)\n",
    "    sat_rate = float(np.mean(np.delete(S, act_idx) >= S[act_idx]))\n",
    "\n",
    "    # margin: min_other_S - S_elim (>= 0 if eliminated is truly worst)\n",
    "    margin = float(np.min(np.delete(S, act_idx)) - S[act_idx])\n",
    "\n",
    "    # worst_rank: rank of actual eliminated in \"worst-first\" order (S smaller = worse)\n",
    "    worst_rank = float(\n",
    "        pd.Series(S).rank(method=\"min\", ascending=True).to_numpy()[act_idx]\n",
    "    )\n",
    "\n",
    "    return pred_name, act_name, sat_rate, margin, worst_rank, len(sub)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load panel and filter usable events\n",
    "    df = load_panel(EST_PATH)\n",
    "    g, usable_keys = get_usable_events(df)\n",
    "\n",
    "    # Run event-level evaluation\n",
    "    event_rows = []\n",
    "    for (s, w) in usable_keys:\n",
    "        sub = g.get_group((s, w))\n",
    "        s_int = int(s)\n",
    "\n",
    "        if s_int in RANK_SUM_SEASONS:\n",
    "            scheme = \"rank_sum\"\n",
    "            pred, act, sat, margin, worst_rank, n_alive = rank_sum_elim(sub)\n",
    "        else:\n",
    "            scheme = \"pct_sum\"\n",
    "            pred, act, sat, margin, worst_rank, n_alive = pct_sum_elim(sub)\n",
    "\n",
    "        event_rows.append(\n",
    "            {\n",
    "                \"season\": s_int,\n",
    "                \"week\": int(w),\n",
    "                \"scheme\": scheme,\n",
    "                \"n_alive\": n_alive,\n",
    "                \"pred_eliminated\": pred,\n",
    "                \"actual_eliminated\": act,\n",
    "                \"hit\": int(pred == act),\n",
    "                \"sat_rate\": sat,\n",
    "                \"margin\": margin,\n",
    "                \"actual_worst_rank(1=worst)\": worst_rank,\n",
    "                \"actual_worst_percentile\": worst_rank / n_alive,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ev = pd.DataFrame(event_rows)\n",
    "\n",
    "    # Overall & by-scheme metrics\n",
    "    print(\"\\n=== Overall consistency ===\")\n",
    "    print(\"Events:\", len(ev))\n",
    "    print(\"Accuracy (hit rate):\", ev[\"hit\"].mean())\n",
    "    print(\"Avg sat_rate:\", ev[\"sat_rate\"].mean())\n",
    "    print(\"Median worst_rank:\", ev[\"actual_worst_rank(1=worst)\"].median())\n",
    "    print(\"Avg worst_percentile:\", ev[\"actual_worst_percentile\"].mean())\n",
    "\n",
    "    print(\"\\n=== By scheme ===\")\n",
    "    print(\n",
    "        ev.groupby(\"scheme\").agg(\n",
    "            events=(\"hit\", \"size\"),\n",
    "            accuracy=(\"hit\", \"mean\"),\n",
    "            avg_sat=(\"sat_rate\", \"mean\"),\n",
    "            median_worst_rank=(\"actual_worst_rank(1=worst)\", \"median\"),\n",
    "            avg_worst_percentile=(\"actual_worst_percentile\", \"mean\"),\n",
    "            median_margin=(\"margin\", \"median\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Season-level consistency (only over usable elimination weeks)\n",
    "    season_summary = (\n",
    "        ev.groupby(\"season\")\n",
    "        .agg(\n",
    "            events=(\"hit\", \"size\"),\n",
    "            accuracy=(\"hit\", \"mean\"),\n",
    "            avg_sat=(\"sat_rate\", \"mean\"),\n",
    "            avg_worst_percentile=(\"actual_worst_percentile\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"accuracy\", \"events\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    # Save outputs\n",
    "    ev.to_csv(OUT_EVENT_PATH, index=False)\n",
    "    season_summary.to_csv(OUT_SEASON_PATH, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\", OUT_EVENT_PATH)\n",
    "    print(\"Saved:\", OUT_SEASON_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90e6fd",
   "metadata": {},
   "source": [
    "## Visualization of Elimination Consistency Diagnostics\n",
    "\n",
    "This script creates a 2×2 panel of plots that summarize how well our reconstructed fan votes align with historical eliminations:\n",
    "\n",
    "1. **Worst percentile distribution (all events)**  \n",
    "   Histogram of the eliminated contestant’s “worst percentile” within their week under the chosen scoring rule (1 = worst in the field).\n",
    "\n",
    "2. **Worst percentile by scoring scheme**  \n",
    "   Overlaid histograms comparing worst percentiles under the Percent-Sum and Rank-Sum schemes.\n",
    "\n",
    "3. **Pairwise satisfaction rate**  \n",
    "   Distribution of the satisfaction rate \\( s_t \\), defined as the share of contestants who are not more “deserving to leave” than the eliminated contestant in week \\( t \\).\n",
    "\n",
    "4. **Season-level accuracy**  \n",
    "   Bar plot of hit rate (prediction accuracy) by season, with a horizontal line showing the overall mean accuracy.\n",
    "\n",
    "The figure is saved as `diagnostics.png` and is suitable for inclusion in the report as a compact visual summary of model–history consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "consistency_plots.py\n",
    "\n",
    "Generate a 2×2 grid of diagnostic plots summarizing how well the\n",
    "reconstructed fan votes reproduce historical eliminations.\n",
    "\n",
    "Requires:\n",
    "    - ev : DataFrame with event-level results\n",
    "        * columns: [\"season\", \"week\", \"scheme\", \"hit\", \"sat_rate\",\n",
    "                    \"actual_worst_percentile\", ...]\n",
    "    - season_summary : DataFrame with season-level accuracy\n",
    "        * columns: [\"season\", \"events\", \"accuracy\", ...]\n",
    "\n",
    "Outputs:\n",
    "    - diagnostics.png : 2×2 panel of consistency plots\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_diagnostics(ev, season_summary, out_path: str = \"diagnostics.png\"):\n",
    "    # ------------------------------------------------------------------\n",
    "    # Global plotting style (clean & paper-friendly)\n",
    "    # ------------------------------------------------------------------\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.size\": 11,\n",
    "            \"axes.titlesize\": 12,\n",
    "            \"axes.labelsize\": 11,\n",
    "            \"xtick.labelsize\": 10,\n",
    "            \"ytick.labelsize\": 10,\n",
    "            \"figure.dpi\": 120,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Create 2×2 subplot layout\n",
    "    # ------------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8))\n",
    "    (ax1, ax2), (ax3, ax4) = axes\n",
    "\n",
    "    # ==============================================================\n",
    "    # (1) Worst percentile distribution (ALL events)\n",
    "    # ==============================================================\n",
    "    ax1.hist(\n",
    "        ev[\"actual_worst_percentile\"],\n",
    "        bins=20,\n",
    "        color=\"#4C72B0\",\n",
    "        edgecolor=\"white\",\n",
    "    )\n",
    "    ax1.axvline(\n",
    "        ev[\"actual_worst_percentile\"].mean(),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    ax1.set_title(\"Worst Percentile of Eliminated Contestant\")\n",
    "    ax1.set_xlabel(\"Worst percentile (1 = worst)\")\n",
    "    ax1.set_ylabel(\"Number of events\")\n",
    "    ax1.legend(frameon=False)\n",
    "\n",
    "    # ==============================================================\n",
    "    # (2) Worst percentile by scheme\n",
    "    # ==============================================================\n",
    "    for scheme, color in zip(\n",
    "        [\"pct_sum\", \"rank_sum\"],\n",
    "        [\"#55A868\", \"#C44E52\"],\n",
    "    ):\n",
    "        sub = ev[ev[\"scheme\"] == scheme]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "\n",
    "        ax2.hist(\n",
    "            sub[\"actual_worst_percentile\"],\n",
    "            bins=20,\n",
    "            alpha=0.6,\n",
    "            label=scheme.replace(\"_\", \" \"),\n",
    "            color=color,\n",
    "            edgecolor=\"white\",\n",
    "        )\n",
    "\n",
    "    ax2.set_title(\"Worst Percentile by Scoring Scheme\")\n",
    "    ax2.set_xlabel(\"Worst percentile\")\n",
    "    ax2.set_ylabel(\"Number of events\")\n",
    "    ax2.legend(frameon=False)\n",
    "\n",
    "    # ==============================================================\n",
    "    # (3) Pairwise satisfaction rate\n",
    "    # ==============================================================\n",
    "    ax3.hist(\n",
    "        ev[\"sat_rate\"],\n",
    "        bins=20,\n",
    "        color=\"#8172B2\",\n",
    "        edgecolor=\"white\",\n",
    "    )\n",
    "    ax3.axvline(\n",
    "        ev[\"sat_rate\"].mean(),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    ax3.set_title(\"Pairwise Satisfaction Rate\")\n",
    "    ax3.set_xlabel(r\"Satisfaction rate $s_t$\")\n",
    "    ax3.set_ylabel(\"Number of events\")\n",
    "    ax3.legend(frameon=False)\n",
    "\n",
    "    # ==============================================================\n",
    "    # (4) Season-level accuracy\n",
    "    # ==============================================================\n",
    "    ax4.bar(\n",
    "        season_summary[\"season\"].astype(str),\n",
    "        season_summary[\"accuracy\"],\n",
    "        color=\"#CCB974\",\n",
    "    )\n",
    "    ax4.axhline(\n",
    "        ev[\"hit\"].mean(),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        label=\"Overall mean\",\n",
    "    )\n",
    "    ax4.set_title(\"Elimination Accuracy by Season\")\n",
    "    ax4.set_xlabel(\"Season\")\n",
    "    ax4.set_ylabel(\"Accuracy\")\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.tick_params(axis=\"x\", rotation=45)\n",
    "    ax4.legend(frameon=False)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Final layout tweaks & save\n",
    "    # ------------------------------------------------------------------\n",
    "    plt.suptitle(\n",
    "        \"Consistency Diagnostics for Reconstructed Fan Votes\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    print(f\"Saved diagnostics figure to: {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Import here to avoid circular dependencies if you structure this as a package.\n",
    "    import pandas as pd\n",
    "\n",
    "    ev = pd.read_csv(\"verification_event_level.csv\")\n",
    "    season_summary = pd.read_csv(\"verification_season_level.csv\")\n",
    "\n",
    "    plot_diagnostics(ev, season_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583e801",
   "metadata": {},
   "source": [
    "## Consistency Check for Reconstructed Fan Votes\n",
    "\n",
    "This script evaluates how well our reconstructed fan vote shares reproduce the **actual weekly eliminations** on *Dancing with the Stars*.\n",
    "\n",
    "**Input**\n",
    "\n",
    "- `fan_shares_estimated.csv`, containing for each (season, week, contestant):\n",
    "  - `season`, `week`\n",
    "  - `celebrity_name`\n",
    "  - `judge_total` (total judges’ score that week)\n",
    "  - `eliminated` (1 if eliminated that week, 0 otherwise)\n",
    "  - `fan_share_hat` (reconstructed fan share from Problem 1)\n",
    "\n",
    "We restrict to weeks where:\n",
    "- exactly **one** contestant is eliminated, and  \n",
    "- at least **two** contestants are still active.\n",
    "\n",
    "---\n",
    "\n",
    "### Elimination Rules Evaluated\n",
    "\n",
    "For each usable (season, week):\n",
    "\n",
    "1. **Rank-Sum rule** (assumed in seasons 1–2 and 28–34):\n",
    "   - Rank contestants by judges’ total and fan share separately (1 = best).\n",
    "   - Sum the two ranks to get a combined rank (smaller = better).\n",
    "   - Predict the eliminated contestant as the one with the **largest** combined rank.\n",
    "\n",
    "2. **Percent-Sum rule** (assumed in seasons 3–27):\n",
    "   - Convert judges’ totals to weekly percentages.\n",
    "   - Treat `fan_share_hat` as fan vote percentage (renormalized within the week).\n",
    "   - Total score = judge percent + fan percent.\n",
    "   - Predict the eliminated contestant as the one with the **smallest** total score.\n",
    "\n",
    "---\n",
    "\n",
    "### Diagnostics and Outputs\n",
    "\n",
    "For each elimination event, we compute:\n",
    "\n",
    "- **`hit`** — whether the predicted elimination matches the actual one (hit rate / accuracy).\n",
    "- **`sat_rate`** — a “pairwise satisfaction” measure:\n",
    "  - Rank-Sum: fraction of other contestants whose combined rank is **no better** than the eliminated contestant’s.\n",
    "  - Percent-Sum: fraction of other contestants whose total score is **no higher** than the eliminated’s.\n",
    "- **`actual_worst_rank(1=worst)`** — how the actual eliminated contestant ranks in a “worst-first” ordering.\n",
    "- **`actual_worst_percentile`** — that rank converted to a percentile within the week.\n",
    "\n",
    "The script writes:\n",
    "\n",
    "- `verification_event_level.csv`  \n",
    "  Event-level diagnostics for every usable (season, week).\n",
    "\n",
    "- `verification_season_level.csv`  \n",
    "  Season-level summaries: number of events, accuracy, mean satisfaction rate, and mean worst percentile.\n",
    "\n",
    "These diagnostics are later used to compare rules and discuss how well our reconstructed fan votes align with historical eliminations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "consistency_check.py\n",
    "\n",
    "Evaluate how consistent the reconstructed fan shares are with the\n",
    "actual elimination outcomes under different combination rules.\n",
    "\n",
    "Input:\n",
    "    fan_shares_estimated.csv\n",
    "        Must contain columns:\n",
    "        - season\n",
    "        - week\n",
    "        - celebrity_name\n",
    "        - judge_total\n",
    "        - eliminated (1 if eliminated this week, else 0)\n",
    "        - fan_share_hat (reconstructed fan share)\n",
    "\n",
    "Output:\n",
    "    verification_event_level.csv\n",
    "        Event-level metrics for each (season, week) elimination event.\n",
    "\n",
    "    verification_season_level.csv\n",
    "        Season-level summary metrics (accuracy, average rank, etc.).\n",
    "\"\"\"\n",
    "\n",
    "# ===== Paths (change if needed) ===========================================\n",
    "EST_PATH = \"fan_shares_estimated.csv\"        # panel with reconstructed fan shares\n",
    "OUT_EVENT_PATH = \"verification_event_level.csv\"\n",
    "OUT_SEASON_PATH = \"verification_season_level.csv\"\n",
    "\n",
    "# Seasons where the actual TV show is believed to have used Rank-Sum\n",
    "RANK_SUM_SEASONS = set([1, 2] + list(range(28, 35)))\n",
    "\n",
    "\n",
    "# ===== Load and basic checks =============================================\n",
    "df = pd.read_csv(EST_PATH)\n",
    "\n",
    "required_cols = {\n",
    "    \"season\", \"week\", \"celebrity_name\",\n",
    "    \"judge_total\", \"eliminated\", \"fan_share_hat\"\n",
    "}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in estimated panel: {missing}\")\n",
    "\n",
    "# Keep only weeks with exactly ONE eliminated contestant and at least 2 active\n",
    "g = df.groupby([\"season\", \"week\"], sort=True)\n",
    "usable_keys = []\n",
    "for (s, w), sub in g:\n",
    "    if sub[\"eliminated\"].sum() == 1 and len(sub) >= 2:\n",
    "        usable_keys.append((s, w))\n",
    "\n",
    "print(\"Usable elimination events:\", len(usable_keys))\n",
    "\n",
    "\n",
    "# ===== Helper: Rank-Sum Rule =============================================\n",
    "def rank_sum_elim(sub: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Rank-Sum rule (as described in the main report):\n",
    "\n",
    "    - Compute separate ranks for judge_total and fan_share_hat (1 = best).\n",
    "    - Sum ranks to get `combined_rank_sum` (smaller is better).\n",
    "    - The eliminated contestant is the one with the LARGEST rank sum.\n",
    "\n",
    "    Returns:\n",
    "        pred_name  (str): predicted eliminated contestant name\n",
    "        act_name   (str): actual eliminated contestant name\n",
    "        sat_rate   (float): fraction of others whose rank sum <= eliminated's\n",
    "        margin     (float): min(other rank sum) - eliminated rank sum\n",
    "        worst_rank (int): actual rank of the eliminated contestant (1 = worst)\n",
    "    \"\"\"\n",
    "    sub = sub.reset_index(drop=True).copy()\n",
    "    j = sub[\"judge_total\"].to_numpy(dtype=float)\n",
    "    f = sub[\"fan_share_hat\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Normalize fan shares for safety\n",
    "    f = f / f.sum() if f.sum() > 0 else np.ones_like(f) / len(f)\n",
    "\n",
    "    # Ranks: 1 = best (largest score/share), n = worst\n",
    "    # So we sort descending and invert\n",
    "    j_order = np.argsort(-j)\n",
    "    f_order = np.argsort(-f)\n",
    "\n",
    "    rank_j = np.empty_like(j_order)\n",
    "    rank_f = np.empty_like(f_order)\n",
    "    rank_j[j_order] = np.arange(1, len(j) + 1)\n",
    "    rank_f[f_order] = np.arange(1, len(f) + 1)\n",
    "\n",
    "    combined = rank_j + rank_f\n",
    "\n",
    "    # Predicted eliminated: largest combined rank (worst)\n",
    "    pred_idx = int(np.argmax(combined))\n",
    "    act_idx = int(np.where(sub[\"eliminated\"].to_numpy() == 1)[0][0])\n",
    "\n",
    "    pred_name = sub.loc[pred_idx, \"celebrity_name\"]\n",
    "    act_name = sub.loc[act_idx, \"celebrity_name\"]\n",
    "\n",
    "    # Diagnostics:\n",
    "    # sat_rate: fraction of other contestants with combined >= eliminated's\n",
    "    sat_rate = float(np.mean(np.delete(combined, act_idx) >= combined[act_idx]))\n",
    "\n",
    "    # margin: min(other combined) - eliminated combined\n",
    "    # (should be >= 0 if eliminated really is worst)\n",
    "    margin = float(np.min(np.delete(combined, act_idx)) - combined[act_idx])\n",
    "\n",
    "    # worst_rank: rank of actual eliminated contestant under \"worst first\"\n",
    "    worst_rank = 1 + np.sum(combined < combined[act_idx])\n",
    "    n_alive = len(sub)\n",
    "    worst_percentile = worst_rank / n_alive\n",
    "\n",
    "    return pred_name, act_name, sat_rate, margin, worst_rank, worst_percentile\n",
    "\n",
    "\n",
    "# ===== Helper: Percent-Sum Rule ==========================================\n",
    "def percent_sum_elim(sub: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Percent-Sum rule:\n",
    "\n",
    "    - Convert judge_total to a judge percent j_pct = J_i / sum_j J_j.\n",
    "    - Use reconstructed fan_share_hat as fan percent (already normalized).\n",
    "    - Total score = j_pct + fan_pct.\n",
    "      Lower score = worse, eliminated = smallest total.\n",
    "\n",
    "    Returns:\n",
    "        pred_name, act_name, sat_rate, margin, worst_rank, worst_percentile\n",
    "        defined analogously to rank_sum_elim.\n",
    "    \"\"\"\n",
    "    sub = sub.reset_index(drop=True).copy()\n",
    "    j = sub[\"judge_total\"].to_numpy(dtype=float)\n",
    "    f = sub[\"fan_share_hat\"].to_numpy(dtype=float)\n",
    "\n",
    "    j_sum = j.sum()\n",
    "    j_pct = (j / j_sum) if j_sum > 0 else (np.ones_like(j) / len(j))\n",
    "\n",
    "    f = f / f.sum() if f.sum() > 0 else np.ones_like(f) / len(f)\n",
    "\n",
    "    S = j_pct + f  # total percent score (higher = better)\n",
    "    # Eliminated = smallest S\n",
    "    pred_idx = int(np.argmin(S))\n",
    "    act_idx = int(np.where(sub[\"eliminated\"].to_numpy() == 1)[0][0])\n",
    "\n",
    "    pred_name = sub.loc[pred_idx, \"celebrity_name\"]\n",
    "    act_name = sub.loc[act_idx, \"celebrity_name\"]\n",
    "\n",
    "    # For sat_rate and margin we interpret S as \"higher is better\"\n",
    "    sat_rate = float(np.mean(np.delete(S, act_idx) >= S[act_idx]))\n",
    "    margin = float(np.min(np.delete(S, act_idx)) - S[act_idx])\n",
    "\n",
    "    # Rank of actual eliminated in \"worst first\" order\n",
    "    worst_rank = 1 + np.sum(S < S[act_idx])\n",
    "    n_alive = len(sub)\n",
    "    worst_percentile = worst_rank / n_alive\n",
    "\n",
    "    return pred_name, act_name, sat_rate, margin, worst_rank, worst_percentile\n",
    "\n",
    "\n",
    "# ===== Main loop over events =============================================\n",
    "event_rows = []\n",
    "\n",
    "for (s, w) in usable_keys:\n",
    "    sub = g.get_group((s, w)).copy()\n",
    "\n",
    "    # Decide which system to simulate for this season\n",
    "    if s in RANK_SUM_SEASONS:\n",
    "        rule = \"rank_sum\"\n",
    "        pred_name, act_name, sat, margin, worst_rank, worst_pct = rank_sum_elim(sub)\n",
    "    else:\n",
    "        rule = \"percent_sum\"\n",
    "        pred_name, act_name, sat, margin, worst_rank, worst_pct = percent_sum_elim(sub)\n",
    "\n",
    "    event_rows.append({\n",
    "        \"season\": s,\n",
    "        \"week\": w,\n",
    "        \"rule\": rule,\n",
    "        \"pred_elim\": pred_name,\n",
    "        \"actual_elim\": act_name,\n",
    "        \"hit\": int(pred_name == act_name),\n",
    "        \"sat_rate\": sat,\n",
    "        \"margin\": margin,\n",
    "        \"actual_worst_rank(1=worst)\": worst_rank,\n",
    "        \"actual_worst_percentile\": worst_pct\n",
    "    })\n",
    "\n",
    "ev = pd.DataFrame(event_rows)\n",
    "\n",
    "\n",
    "# ===== Overall & per-season metrics ======================================\n",
    "print(\"\\n=== Overall consistency ===\")\n",
    "print(\"Events:\", len(ev))\n",
    "print(\"Accuracy (hit rate):\", ev[\"hit\"].mean())\n",
    "print(\"Avg sat_rate:\", ev[\"sat_rate\"].mean())\n",
    "print(\"Median worst percentile:\", ev[\"actual_worst_percentile\"].median())\n",
    "\n",
    "print(\"\\n=== By rule (rank_sum vs percent_sum) ===\")\n",
    "print(ev.groupby(\"rule\")[\"hit\"].mean())\n",
    "\n",
    "# Per-season consistency (only over usable elimination weeks)\n",
    "season_summary = (\n",
    "    ev.groupby(\"season\")\n",
    "      .agg(\n",
    "          events=(\"hit\", \"size\"),\n",
    "          accuracy=(\"hit\", \"mean\"),\n",
    "          avg_sat=(\"sat_rate\", \"mean\"),\n",
    "          avg_worst_percentile=(\"actual_worst_percentile\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"accuracy\", \"events\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# Save results\n",
    "ev.to_csv(OUT_EVENT_PATH, index=False)\n",
    "season_summary.to_csv(OUT_SEASON_PATH, index=False)\n",
    "\n",
    "print(\"\\nSaved:\", OUT_EVENT_PATH)\n",
    "print(\"Saved:\", OUT_SEASON_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b589ccf",
   "metadata": {},
   "source": [
    "## Certainty of Elimination Predictions (Fan + Judge Model)\n",
    "\n",
    "This script takes the fitted output from `fan_shares_estimated.csv` and quantifies **how certain** our fan+judge model is about each weekly elimination.\n",
    "\n",
    "### Input\n",
    "\n",
    "`fan_shares_estimated.csv`, containing for each (season, week, contestant):\n",
    "\n",
    "- `season`, `week`\n",
    "- `celebrity_name`\n",
    "- `judge_total`\n",
    "- `elim_week`, `eliminated`\n",
    "- `fan_share_hat` (estimated fan share \\(F_{it}\\))\n",
    "- plus helper columns (`judge_std`, `contestant_id`, `week_float`)\n",
    "\n",
    "We assume this file already reflects the fitted fan model from Problem 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Model for Elimination Certainty\n",
    "\n",
    "For each week \\( t \\) with active contestant set \\( C_t \\), the script:\n",
    "\n",
    "1. Computes **judge percentage**:\n",
    "   \\[\n",
    "   J^{\\%}_{it} = \\frac{\\text{judge\\_total}_{it}}{\\sum_{j \\in C_t} \\text{judge\\_total}_{jt}}.\n",
    "   \\]\n",
    "\n",
    "2. Defines a **total score** combining judges and fans:\n",
    "   \\[\n",
    "   S_{it} = J^{\\%}_{it} + F_{it},\n",
    "   \\]\n",
    "   where \\( F_{it} = \\text{fan\\_share\\_hat}_{it} \\).\n",
    "\n",
    "3. For each contestant \\( i \\in C_t \\), constructs a **pairwise logistic weight**:\n",
    "   \\[\n",
    "   w_{it} = \\prod_{j \\in C_t, j \\neq i} \\sigma\\big(\\alpha (S_{jt} - S_{it})\\big),\n",
    "   \\]\n",
    "   where \\( \\sigma(\\cdot) \\) is the logistic function and `ALPHA_SOFT` controls how “hard” the comparison is (higher \\(\\alpha\\) → closer to a sharp indicator of \\(S_j > S_i\\)).\n",
    "\n",
    "4. Normalizes these weights within the week to obtain a **probability that contestant \\( i \\) is eliminated**:\n",
    "   \\[\n",
    "   p_{it} = \\frac{w_{it}}{\\sum_{k \\in C_t} w_{kt}}.\n",
    "   \\]\n",
    "\n",
    "For each actual elimination, \\( p_{it} \\) evaluated at the eliminated contestant is interpreted as the **certainty** of that elimination under our model.\n",
    "\n",
    "---\n",
    "\n",
    "### Certainty Metrics Produced\n",
    "\n",
    "From the week-by-week probabilities, the script reports:\n",
    "\n",
    "- **Event-level accuracy**:  \n",
    "  Fraction of weeks where the contestant with the **highest elimination probability** \\( p_{it} \\) is indeed the one eliminated.\n",
    "\n",
    "- **Certainty of observed eliminations**:\n",
    "  - Mean and median \\( p_{i_t^* t} \\), where \\( i_t^* \\) is the actual eliminated contestant.\n",
    "  - 25th/75th percentiles, min and max of these probabilities.\n",
    "\n",
    "- **Log-likelihood of the elimination path**:\n",
    "  \\[\n",
    "  \\sum_t \\log p_{i_t^* t},\n",
    "  \\]\n",
    "  summarizing how plausible the entire observed sequence of eliminations is under the model.\n",
    "\n",
    "- **Uniform baseline certainty**:\n",
    "  Mean of \\( 1 / |C_t| \\) across weeks, representing the certainty level if eliminations were random among active contestants.\n",
    "\n",
    "Additionally, the script prints:\n",
    "\n",
    "- A small **LaTeX table snippet** with key summary statistics (ready to paste into the paper).\n",
    "- The **top 10 most “certain” eliminations** (largest \\( p_{i_t^* t} \\)).\n",
    "- The **bottom 10 most “surprising” eliminations** (smallest \\( p_{i_t^* t} \\)), which are natural candidates for case studies and discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluate_fan_certainty.py\n",
    "\n",
    "Use the already-estimated fan model output in fan_shares_estimated.csv\n",
    "to compute *certainty* of eliminations.\n",
    "\n",
    "INPUT: fan_shares_estimated.csv with columns like:\n",
    "    - season\n",
    "    - celebrity_name\n",
    "    - week\n",
    "    - judge_total\n",
    "    - elim_week        (NaN for finalists / winners)\n",
    "    - eliminated       (1 if eliminated in that week, 0 otherwise)\n",
    "    - judge_std        (not used here, but present)\n",
    "    - contestant_id\n",
    "    - week_float\n",
    "    - fan_share_hat    (our estimated F_it from the fan model)\n",
    "\n",
    "STEPS:\n",
    "    For each (season, week) alive set C_t:\n",
    "\n",
    "    1) Judge score percentage:\n",
    "           J_pct_it = judge_total_it / sum_j judge_total_jt\n",
    "\n",
    "    2) Total score:\n",
    "           S_it = J_pct_it + fan_share_hat_it\n",
    "\n",
    "    3) Raw elimination weight:\n",
    "           w_it = ∏_{j∈C_t, j≠i} σ( α * (S_jt - S_it) )\n",
    "\n",
    "    4) Proper probability:\n",
    "           p_it = w_it / sum_{k∈C_t} w_kt\n",
    "\n",
    "    5) Certainty metrics:\n",
    "        - log-likelihood over observed eliminations\n",
    "        - mean / median p_it for actually eliminated contestants\n",
    "        - event-level accuracy (how often argmax p_it is eliminated?)\n",
    "\n",
    "Tune ALPHA_SOFT to control how \"hard\" the pairwise comparison is.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit  # logistic σ(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. Configuration\n",
    "# ============================================================\n",
    "\n",
    "FAN_CSV_PATH = \"fan_shares_estimated.csv\"\n",
    "\n",
    "# Softness parameter in pairwise logistic comparison.\n",
    "# Larger -> closer to hard \"S_j > S_i\" indicator.\n",
    "ALPHA_SOFT = 20.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Core computations\n",
    "# ============================================================\n",
    "\n",
    "def add_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the fan_shares_estimated dataframe, add:\n",
    "\n",
    "        - J_pct_it   = judge percentage in each (season, week)\n",
    "        - S_it       = J_pct_it + fan_share_hat\n",
    "\n",
    "    We do NOT refit the fan model; we just use fan_share_hat as F_it.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Judge percentage within each (season, week)\n",
    "    total_J = df.groupby([\"season\", \"week\"])[\"judge_total\"].transform(\"sum\")\n",
    "    df[\"J_pct_it\"] = df[\"judge_total\"] / total_J\n",
    "\n",
    "    # Total score S_it = J_pct + F_it\n",
    "    df[\"S_it\"] = df[\"J_pct_it\"] + df[\"fan_share_hat\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_week_probs(sub: pd.DataFrame, alpha_soft: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a single (season, week) subset, compute raw weights w_it and\n",
    "    normalized probabilities p_elim for all contestants in that week.\n",
    "\n",
    "    sub must already have S_it.\n",
    "    \"\"\"\n",
    "    sub = sub.copy()\n",
    "    S = sub[\"S_it\"].to_numpy()\n",
    "\n",
    "    n = len(S)\n",
    "    # Matrix of α(S_j - S_i): shape (n, n), row i, col j.\n",
    "    diff_mat = alpha_soft * (S[None, :] - S[:, None])\n",
    "    sig_mat = expit(diff_mat)  # σ(α(S_j - S_i))\n",
    "\n",
    "    # Exclude i=j from the product by setting diagonal = 1\n",
    "    np.fill_diagonal(sig_mat, 1.0)\n",
    "\n",
    "    # Raw weights: product over j≠i\n",
    "    w = sig_mat.prod(axis=1)\n",
    "\n",
    "    # Normalize to get a proper distribution over contestants in this week\n",
    "    w_sum = w.sum()\n",
    "    if w_sum <= 0:\n",
    "        p = np.ones_like(w) / n\n",
    "    else:\n",
    "        p = w / w_sum\n",
    "\n",
    "    sub[\"w_it\"] = w\n",
    "    sub[\"p_elim\"] = p\n",
    "\n",
    "    return sub\n",
    "\n",
    "\n",
    "def compute_all_probs(df: pd.DataFrame, alpha_soft: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loop over all (season, week) and compute p_elim for everyone.\n",
    "\n",
    "    We treat every row in fan_shares_estimated as a contestant who is\n",
    "    alive in that week (the CSV only contains weeks when they danced).\n",
    "    \"\"\"\n",
    "    df = add_scores(df)\n",
    "\n",
    "    results = []\n",
    "    for (season, week), sub in df.groupby([\"season\", \"week\"], sort=True):\n",
    "        res_week = compute_week_probs(sub, alpha_soft)\n",
    "        results.append(res_week)\n",
    "\n",
    "    out = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Flag the rows corresponding to actual eliminations\n",
    "    out[\"is_eliminated_this_week\"] = out[\"eliminated\"] == 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Certainty metrics\n",
    "# ============================================================\n",
    "def summarize_certainty(df_probs: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Given df_probs with p_elim and is_eliminated_this_week, compute:\n",
    "\n",
    "        - log-likelihood over observed eliminations\n",
    "        - average and median p_elim for eliminated contestants (Certainty events)\n",
    "        - event-level accuracy (argmax p_elim is eliminated?)\n",
    "        - a uniform-baseline certainty for comparison\n",
    "    \"\"\"\n",
    "    # All actual elimination *rows* (each row = one elimination event)\n",
    "    elim_rows = df_probs[df_probs[\"is_eliminated_this_week\"]].copy()\n",
    "\n",
    "    # -------- 1. Certainty_t = p_elim of eliminated contestant --------\n",
    "    eps = 1e-12\n",
    "    cert_values = np.clip(elim_rows[\"p_elim\"].to_numpy(), eps, 1.0)\n",
    "\n",
    "    loglik = np.log(cert_values).sum()\n",
    "    avg_p = cert_values.mean()\n",
    "    med_p = np.median(cert_values)\n",
    "\n",
    "    # Optional distribution summary if you want it later\n",
    "    q25, q75 = np.quantile(cert_values, [0.25, 0.75])\n",
    "    p_min, p_max = cert_values.min(), cert_values.max()\n",
    "\n",
    "    # -------- 2. Event-level accuracy (hit rate) --------\n",
    "    correct_events = 0\n",
    "    total_events = 0\n",
    "\n",
    "    for (season, week), sub in df_probs.groupby([\"season\", \"week\"]):\n",
    "        if not sub[\"is_eliminated_this_week\"].any():\n",
    "            continue  # no elimination in this (season, week) combo\n",
    "\n",
    "        total_events += 1\n",
    "        max_p = sub[\"p_elim\"].max()\n",
    "        top = sub[sub[\"p_elim\"] == max_p]\n",
    "\n",
    "        if top[\"is_eliminated_this_week\"].any():\n",
    "            correct_events += 1\n",
    "\n",
    "    event_accuracy = correct_events / total_events if total_events > 0 else np.nan\n",
    "\n",
    "    # -------- 3. Uniform baseline for certainty --------\n",
    "    # For each elimination event, baseline certainty would be 1 / |C_t|\n",
    "    # where |C_t| is the number of contestants alive in that week.\n",
    "    # We approximate this by the size of the group in df_probs.\n",
    "    baseline_certs = []\n",
    "    for (season, week), sub in df_probs.groupby([\"season\", \"week\"]):\n",
    "        if not sub[\"is_eliminated_this_week\"].any():\n",
    "            continue\n",
    "        n_alive = len(sub)\n",
    "        baseline_certs.extend([1.0 / n_alive] * sub[\"is_eliminated_this_week\"].sum())\n",
    "\n",
    "    baseline_certs = np.array(baseline_certs, dtype=float)\n",
    "    baseline_mean = baseline_certs.mean() if baseline_certs.size > 0 else np.nan\n",
    "\n",
    "    stats = {\n",
    "        \"log_likelihood\": loglik,\n",
    "        \"avg_p_elim_eliminated\": avg_p,      # mean Certainty_t\n",
    "        \"median_p_elim_eliminated\": med_p,   # median Certainty_t\n",
    "        \"q25_p_elim_eliminated\": q25,\n",
    "        \"q75_p_elim_eliminated\": q75,\n",
    "        \"min_p_elim_eliminated\": p_min,\n",
    "        \"max_p_elim_eliminated\": p_max,\n",
    "        \"event_accuracy\": event_accuracy,\n",
    "        \"num_elimination_events\": len(elim_rows),\n",
    "        \"num_elimination_weeks\": total_events,\n",
    "        \"baseline_mean_certainty\": baseline_mean,\n",
    "    }\n",
    "\n",
    "    return stats, elim_rows\n",
    "\n",
    "# ============================================================\n",
    "# 3. Main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # Load the fan model output\n",
    "    df = pd.read_csv(FAN_CSV_PATH)\n",
    "\n",
    "    # Compute week-by-week elimination probabilities implied by fan model\n",
    "    df_probs = compute_all_probs(df, ALPHA_SOFT)\n",
    "\n",
    "    # Summarize certainty\n",
    "    stats, elim_rows = summarize_certainty(df_probs)\n",
    "\n",
    "    print(\"=== Certainty of the fan+judge model (based on fan_shares_estimated.csv) ===\")\n",
    "    print(f\"Alpha_soft (pairwise softness):              {ALPHA_SOFT}\")\n",
    "    print(f\"Number of elimination events (rows):         {stats['num_elimination_events']}\")\n",
    "    print(f\"Number of elimination weeks (unique weeks):  {stats['num_elimination_weeks']}\")\n",
    "    print(f\"Event-level accuracy (hit rate):             {stats['event_accuracy']:.3f}\")\n",
    "    print(f\"Avg p_elim for eliminated rows:              {stats['avg_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"Median p_elim for eliminated rows:           {stats['median_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"25/75% quantiles of p_elim (elim rows):      {stats['q25_p_elim_eliminated']:.3f}, {stats['q75_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"Min / Max p_elim for eliminated rows:        {stats['min_p_elim_eliminated']:.3e}, {stats['max_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"Log-likelihood of elimination path:          {stats['log_likelihood']:.3f}\")\n",
    "    print(f\"Uniform baseline mean certainty:             {stats['baseline_mean_certainty']:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # ---------- Paper-ready \"TBD\" block ----------\n",
    "    print(\"=== Values to fill in TBDs in the paper ===\")\n",
    "    print(f\"Mean Certainty_t (avg p_elim of eliminated): {stats['avg_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"Median Certainty_t:                          {stats['median_p_elim_eliminated']:.3f}\")\n",
    "    print(f\"Event-level accuracy (hit rate):             {stats['event_accuracy']:.3f}\")\n",
    "    print(f\"Log-likelihood (sum log Certainty_t):        {stats['log_likelihood']:.3f}\")\n",
    "    print(f\"Mean uniform-baseline certainty:             {stats['baseline_mean_certainty']:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # ---------- Optional LaTeX table snippet ----------\n",
    "    print(\"=== LaTeX snippet for Table~\\\\ref{tab:certainty} (edit caption/label as needed) ===\")\n",
    "    print(r\"\\\\begin{tabular}{l c}\")\n",
    "    print(r\"\\\\hline\")\n",
    "    print(r\"Number of elimination events & %d \\\\\\\\\" % stats[\"num_elimination_events\"])\n",
    "    print(r\"Event-level accuracy & %.3f \\\\\\\\\" % stats[\"event_accuracy\"])\n",
    "    print(r\"Mean certainty $\\\\mathbb{E}[p_{i_t^* t}]$ & %.3f \\\\\\\\\" % stats[\"avg_p_elim_eliminated\"])\n",
    "    print(r\"Median certainty & %.3f \\\\\\\\\" % stats[\"median_p_elim_eliminated\"])\n",
    "    print(r\"Uniform baseline (mean $1/|C_t|$) & %.3f \\\\\\\\\" % stats[\"baseline_mean_certainty\"])\n",
    "    print(r\"Log-likelihood $\\\\sum_t \\\\log p_{i_t^* t}$ & %.3f \\\\\\\\\" % stats[\"log_likelihood\"])\n",
    "    print(r\"\\\\hline\")\n",
    "    print(r\"\\\\end{tabular}\")\n",
    "    print()\n",
    "\n",
    "    # ---------- Examples of individual events ----------\n",
    "    print(\"Top 10 most 'certain' eliminations (highest p_elim):\")\n",
    "    top10 = elim_rows.sort_values(\"p_elim\", ascending=False).head(10)\n",
    "    print(\n",
    "        top10[\n",
    "            [\"season\", \"week\", \"celebrity_name\", \"p_elim\", \"judge_total\",\n",
    "             \"fan_share_hat\", \"J_pct_it\", \"S_it\"]\n",
    "        ].to_string(index=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nBottom 10 most 'surprising' eliminations (lowest p_elim):\")\n",
    "    bottom10 = elim_rows.sort_values(\"p_elim\", ascending=True).head(10)\n",
    "    print(\n",
    "        bottom10[\n",
    "            [\"season\", \"week\", \"celebrity_name\", \"p_elim\", \"judge_total\",\n",
    "             \"fan_share_hat\", \"J_pct_it\", \"S_it\"]\n",
    "        ].to_string(index=False)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
